{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0eb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3de84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_types = pathlib.Path('/home/loupy/code/Loupy2023/get_it_right/raw_data/waste_types')\n",
    "garb_cardboard = list(waste_types.glob('cardboard/*'))\n",
    "garb_glass = list(waste_types.glob('glass/*'))\n",
    "garb_paper = list(waste_types.glob('paper/*'))\n",
    "garb_metal = list(waste_types.glob('metal/*'))\n",
    "garb_plastic = list(waste_types.glob('plastic/*'))\n",
    "garb_trash = list(waste_types.glob('trash/*'))\n",
    "img = Image.open(str(garb_cardboard[100]))\n",
    "img_height, img_width =  img.size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffb3eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2d9e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af76220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2390 files belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 18:11:59.065257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-08 18:11:59.070867: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 18:11:59.071527: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-603AUFB): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 18:11:59.182094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardboard', 'glass', 'metal', 'paper', 'plastic']\n"
     ]
    }
   ],
   "source": [
    "data = image_dataset_from_directory(waste_types)\n",
    "class_names = data.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e257b",
   "metadata": {},
   "source": [
    "# Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31569cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in /home/loupy/.pyenv/versions/3.10.6/lib/python3.10/site-packages (0.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0c3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "dataset_url = pathlib.Path('/home/loupy/code/Loupy2023/get_it_right/raw_data/waste_types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f54b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_height = 512\n",
    "img_width = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71b6262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2390 files belonging to 5 classes.\n",
      "Using 1912 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_url,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4270f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2390 files belonging to 5 classes.\n",
      "Using 478 files for validation.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_url,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12930aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f62f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:46:08.515250: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 37748736 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 512, 384, 3)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "train_ds\n",
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2723e",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1badf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pre-processing layers\n",
    "from tensorflow.keras import layers\n",
    "shape=(img_height, img_width, 3)\n",
    "shape_def = (batch_size, img_height, img_width, 3)\n",
    "\n",
    "\n",
    "#preprocessing_layers = tf.keras.Sequential([\n",
    "    #layers.experimental.preprocessing.Rescaling(1./255),\n",
    "# Define the model architecture\n",
    "\n",
    "def initialize_model():\n",
    "   # model = Sequential([\n",
    "        #layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "#         model.add(Conv2D(5, 3, padding= 'same', activation='relu', input_shape=shape)),\n",
    "#         model.add(MaxPooling2D()),\n",
    "#         model.add(Conv2D(16, 3,padding= 'same', activation='relu')),\n",
    "#         model.add(MaxPooling2D()),\n",
    "#         model.addFlatten()\n",
    "#         model.add(Dense(5, activation='relu'))\n",
    "#         model.add(Dense(5, activation='softmax'))\n",
    "        \n",
    "    model = Sequential([\n",
    "      layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "      layers.RandomFlip(\"horizontal\"),\n",
    "      layers.RandomRotation(0.1),\n",
    "      #layers.RandomCrop(input_shape[0], input_shape[1]),\n",
    "      layers.Conv2D(5, 3, padding='same', activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(10, activation='relu'),\n",
    "      layers.Dense(5)\n",
    "    ])  \n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "       # model.build(input_shape=shape_def)\n",
    "    \n",
    "# Compile the model\n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59397ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49c63caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 512, 384, 3)       0         \n",
      "                                                                 \n",
      " random_flip_1 (RandomFlip)  (None, 512, 384, 3)       0         \n",
      "                                                                 \n",
      " random_rotation_1 (RandomRo  (None, 512, 384, 3)      0         \n",
      " tation)                                                         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 512, 384, 5)       140       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 256, 192, 5)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 256, 192, 16)      736       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 128, 96, 16)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 196608)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1966090   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,967,021\n",
      "Trainable params: 1,967,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "427671b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 18:31:04.272455: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 37748736 exceeds 10% of free system memory.\n",
      "2023-03-08 18:31:36.269088: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 37748736 exceeds 10% of free system memory.\n",
      "2023-03-08 18:31:38.268288: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 37748736 exceeds 10% of free system memory.\n",
      "2023-03-08 18:31:41.518322: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 37748736 exceeds 10% of free system memory.\n",
      "2023-03-08 18:31:42.014509: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 62914560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1188s 9s/step - loss: 1.7953 - accuracy: 0.2317 - val_loss: 1.6058 - val_accuracy: 0.2615\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=test_ds,\n",
    "  epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67d74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "net = cv2.dnn.readNetFromTensorflow('faster_rcnn.pb')\n",
    "# Load the input image\n",
    "image = cv2.imread('image.jpg')\n",
    "# Create a blob from the input image\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(300, 300), mean=(0, 0, 0), swapRB=True, crop=False)\n",
    "# Set the input for the network\n",
    "net.setInput(blob)\n",
    "# Run the forward pass to get the output of the network\n",
    "detections = net.forward()\n",
    "# Loop over the detections\n",
    "for i in range(detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    if confidence > 0.5:\n",
    "        # Get the coordinates of the bounding box\n",
    "        box = detections[0, 0, i, 3:7] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        # Draw the bounding box and label on the image\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "        label = '{0:.2f}%'.format(confidence * 100)\n",
    "        cv2.putText(image, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "# Show the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
